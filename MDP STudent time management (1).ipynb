{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64319dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f06b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [17.649250271320458 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.27538983206195156\n",
      "State: [18.698557423825992 0.0 5 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.12249961394125375\n",
      "State: [20.742724859206668 0.0 1 'Excellent' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.8854140103427115\n",
      "State: [13.35899785137698 0.0 5 'Excellent' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.7890007633698047\n",
      "State: [15.98621692010918 0.0 3 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.539945336126654\n",
      "State: [13.8392496845942 0.0 5 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.03412785051947398\n",
      "State: [14.508230444044424 0.0 5 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.33147351804528447\n",
      "State: [20.719774174359557 0.0 5 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.5057942057883051\n",
      "State: [15.953654250434765 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.25639337207505797\n",
      "State: [15.600213401657506 0.0 3 'Excellent' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.419689791887186\n",
      "State: [15.549291804681973 0.0 5 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.24173837561046818\n",
      "State: [14.191911350357056 0.0 5 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.5268695914105368\n",
      "State: [14.06818206733263 0.0 5 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.8133525656886988\n",
      "State: [15.3003164395886 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.35659707363227855\n",
      "State: [14.577308044356906 0.0 5 'Excellent' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.30670136601644193\n",
      "State: [19.834199388468885 0.0 1 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.5866712189951865\n",
      "State: [19.834796466988013 0.0 5 'Poor' 5.0 1.152859081893901], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.8846447561827301\n",
      "State: [13.63325431546928 0.0 5 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.9942603730226437\n",
      "State: [16.264135884402183 0.0 3 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.04252733722963287\n",
      "State: [15.158085925756144 0.0 3 'Poor' 5.0 1.6483819193763865], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.007969453911754187\n",
      "State: [13.948163651872497 0.0 3 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.08415866988666865\n",
      "State: [17.056164976240566 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.8703007545009331\n",
      "State: [18.727129835017838 0.0 3 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.13822965744325788\n",
      "State: [19.723717217004257 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.05506057130731967\n",
      "State: [17.980193432198327 0.0 5 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.02850227500082292\n",
      "State: [15.640406026665504 0.0 5 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.42180575157771394\n",
      "State: [14.789363921480852 0.0 5 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.26718005572696624\n",
      "State: [20.37892026227509 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.7712818320243855\n",
      "State: [20.778644888456853 0.0 3 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.5120117683385489\n",
      "State: [13.977396827954076 0.0 3 'Excellent' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.21300414992476202\n",
      "State: [18.917535035382706 0.0 1 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.1546419771832761\n",
      "State: [13.93156057578144 0.0 5 'Excellent' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.9582937985518265\n",
      "State: [16.610466764848884 0.0 1 'Poor' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.40531644982375625\n",
      "State: [12.23767906521775 0.0 1 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.11646719889327484\n",
      "State: [12.750071710485471 0.0 5 'Poor' 5.0 1.5474362916075106], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.8125059673528255\n",
      "State: [20.133651941973653 0.0 1 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.3065147648882728\n",
      "State: [14.21253401274826 0.0 3 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.218971555966881\n",
      "State: [20.06777289517444 0.0 3 'Good' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Break Time, Reward: 0.7028042749366082\n",
      "State: [15.05404055469767 0.0 3 'Average' 5.0 1.0], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.24216188035888586\n",
      "State: [12.45552624323464 0.0 1 'Poor' 5.0 1.355418188996976], Optimal Action: Increase Study Time, Expected Action: Increase Study Time, Reward: 0.37126746124064414\n",
      "\n",
      "Global Accuracy: 0.7\n",
      "Execution Time: 0.012002229690551758 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('finaldata.csv')\n",
    "\n",
    "# Extract the relevant columns as states\n",
    "states = data[['academic_load', 'current_grade_average', 'time_of_semester', 'mental_state', 'sleep_time', 'break_time']].values.tolist()\n",
    "\n",
    "# Define the actions\n",
    "actions = ['Increase Study Time', 'Decrease Study Time', 'Increase Break Time', 'Decrease Break Time',\n",
    "           'Increase Sleep Time', 'Decrease Sleep Time', 'Increase Social Time', 'Decrease Social Time']\n",
    "\n",
    "def calculate_transition_probabilities(data):\n",
    "    # Calculate transition probabilities based on the dataset\n",
    "    transitions = {}\n",
    "\n",
    "    for state in data.values:\n",
    "        state_tuple = tuple(state)\n",
    "        transitions[state_tuple] = {}\n",
    "\n",
    "        current_state_data = data[\n",
    "            (data['academic_load'] == state[0]) &\n",
    "            (data['current_grade_average'] == state[1]) &\n",
    "            (data['time_of_semester'] == state[2]) &\n",
    "            (data['mental_state'] == state[3])\n",
    "        ]\n",
    "\n",
    "        action_counts = current_state_data['actions'].value_counts()\n",
    "        total_count = action_counts.sum()\n",
    "\n",
    "        for action in actions:\n",
    "            if action in action_counts:\n",
    "                transition_prob = action_counts[action] / total_count\n",
    "            else:\n",
    "                transition_prob = 0.0\n",
    "            transitions[state_tuple][action] = transition_prob\n",
    "\n",
    "    # Convert transition probabilities to a 3D array\n",
    "    num_states = len(states)\n",
    "    num_actions = len(actions)\n",
    "    transition_probabilities = np.zeros((num_actions, num_states, num_states))\n",
    "\n",
    "    for state_idx, state in enumerate(states):\n",
    "        state_tuple = tuple(state)\n",
    "        for action_idx, action in enumerate(actions):\n",
    "            transition_probabilities[action_idx, state_idx, :] = [\n",
    "                transitions.get(state_tuple, {}).get(action, 0.0)\n",
    "                for _ in range(num_states)\n",
    "            ]\n",
    "\n",
    "    return transition_probabilities\n",
    "\n",
    "\n",
    "def calculate_rewards(training_data):\n",
    "    num_states = len(states)\n",
    "    num_actions = len(actions)\n",
    "    rewards = np.random.rand(num_actions, num_states)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, transition_probabilities, rewards, discount_factor=0.9):\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "        self.rewards = rewards\n",
    "        self.discount_factor = discount_factor\n",
    "    \n",
    "    def solve(self):\n",
    "        num_states = len(states)\n",
    "        num_actions = len(actions)\n",
    "        \n",
    "        # Perform value iteration\n",
    "        values = np.zeros(num_states)\n",
    "        policy = np.zeros(num_states, dtype=int)\n",
    "        \n",
    "        while True:\n",
    "            new_values = np.zeros(num_states)\n",
    "            \n",
    "            for state in range(num_states):\n",
    "                q_values = np.zeros(num_actions)\n",
    "                \n",
    "                for action in range(num_actions):\n",
    "                    q_value = 0.0\n",
    "                    \n",
    "                    for next_state in range(num_states):\n",
    "                        q_value += self.transition_probabilities[action, state, next_state] * \\\n",
    "                                   (self.rewards[action, state] + self.discount_factor * values[next_state])\n",
    "                    \n",
    "                    q_values[action] = q_value\n",
    "                \n",
    "                new_values[state] = np.max(q_values)\n",
    "                policy[state] = np.argmax(q_values)\n",
    "            \n",
    "            if np.max(np.abs(new_values - values)) < 1e-8:\n",
    "                break\n",
    "            \n",
    "            values = new_values\n",
    "        \n",
    "        self.values = values\n",
    "        self.policy = policy\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the transition probabilities using the training data\n",
    "transition_probabilities = calculate_transition_probabilities(train_data)\n",
    "\n",
    "# Define the reward function using the training data\n",
    "rewards = calculate_rewards(train_data)\n",
    "\n",
    "# Create the MDP model\n",
    "mdp_model = MDP(transition_probabilities, rewards)\n",
    "\n",
    "# Solve the MDP\n",
    "mdp_model.solve()\n",
    "\n",
    "# Retrieve the optimal policy\n",
    "optimal_policy = mdp_model.policy\n",
    "\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "start_time = time.time()\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for index, test_row in test_data.iterrows():\n",
    "    state = test_row[['academic_load', 'current_grade_average', 'time_of_semester', 'mental_state', 'sleep_time', 'break_time']].values\n",
    "    \n",
    "    # Get the expected action from the test data\n",
    "    expected_action = test_row['actions']\n",
    "    \n",
    "    # Get the optimal action predicted by the model\n",
    "    optimal_action = actions[optimal_policy[index]]\n",
    "    \n",
    "    # Get the index of the optimal action\n",
    "    optimal_action_index = actions.index(optimal_action)\n",
    "    \n",
    "    # Get the reward for the optimal action and state\n",
    "    reward = rewards[optimal_action_index, index]\n",
    "    \n",
    "    # Compare the predicted action with the expected action\n",
    "    if optimal_action == expected_action:\n",
    "        correct_predictions += 1\n",
    "    \n",
    "    total_predictions += 1\n",
    "    \n",
    "    print(f\"State: {state}, Optimal Action: {optimal_action}, Expected Action: {expected_action}, Reward: {reward}\")\n",
    "    \n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "# Calculate global accuracy\n",
    "global_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f\"\\nGlobal Accuracy: {global_accuracy}\")\n",
    "print(f\"Execution Time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f20ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
